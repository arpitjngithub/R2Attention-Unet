{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sprasad/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 4488/4488 [00:51<00:00, 86.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SIZE = 256\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = cv2.imread(img_name, 1)  # Change 0 to 1 for color images\n",
    "        image = cv2.resize(image, (SIZE, SIZE))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load images\n",
    "image_dir = '/data1/sprasad/data/unlabeled'\n",
    "dataset = ImageDataset(image_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "img_data = []\n",
    "for img in tqdm(dataloader):\n",
    "    img_data.append(img.numpy())\n",
    "\n",
    "img_array = np.concatenate(img_data, axis=0)\n",
    "img_array = img_array.astype('float32') / 255.\n",
    "img_array=torch.tensor(img_array).float()\n",
    "\n",
    "# # In the interest of time let us train on 500 images\n",
    "# img_array2 = torch.tensor(img_array[200:700]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = ConvBlock(out_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.upconv(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc1 = EncoderBlock(in_channels, 64)\n",
    "        self.enc2 = EncoderBlock(64, 128)\n",
    "        self.enc3 = EncoderBlock(128, 256)\n",
    "        self.enc4 = EncoderBlock(256, 512)\n",
    "        self.bridge = ConvBlock(512, 1024)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        s1, p1 = self.enc1(x)\n",
    "        s2, p2 = self.enc2(p1)\n",
    "        s3, p3 = self.enc3(p2)\n",
    "        s4, p4 = self.enc4(p3)\n",
    "        b = self.bridge(p4)\n",
    "        return s1, s2, s3, s4, b\n",
    "\n",
    "class AutoencoderDecoder(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super(AutoencoderDecoder, self).__init__()\n",
    "        self.dec1 = DecoderBlock(1024, 512)\n",
    "        self.dec2 = DecoderBlock(512, 256)\n",
    "        self.dec3 = DecoderBlock(256, 128)\n",
    "        self.dec4 = DecoderBlock(128, 64)\n",
    "        self.output = nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dec1(x)\n",
    "        x = self.dec2(x)\n",
    "        x = self.dec3(x)\n",
    "        x = self.dec4(x)\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(in_channels)\n",
    "        self.decoder = AutoencoderDecoder(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, _, _, _, encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# # Hyperparameters\n",
    "# epochs = 800\n",
    "# learning_rate = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "# # Loss and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# autoencoder.train()\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_loss = 0\n",
    "#     for img in tqdm(img_array):\n",
    "#         img = img.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = autoencoder(img.unsqueeze(0))\n",
    "#         loss = criterion(output, img.unsqueeze(0))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "#     print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(img_array)}')\n",
    "#     torch.save(autoencoder.state_dict(),f'auto_encoder_wts/pretrained_wt_unet_model_epoch_{epoch+1}.pth')\n",
    "    \n",
    "# # Save the model\n",
    "\n",
    "# Initialize the autoencoder\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder = Autoencoder(in_channels=3, out_channels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (enc1): EncoderBlock(\n",
       "      (conv): ConvBlock(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (enc2): EncoderBlock(\n",
       "      (conv): ConvBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (enc3): EncoderBlock(\n",
       "      (conv): ConvBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (enc4): EncoderBlock(\n",
       "      (conv): ConvBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (bridge): ConvBlock(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): AutoencoderDecoder(\n",
       "    (dec1): DecoderBlock(\n",
       "      (upconv): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv): ConvBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (dec2): DecoderBlock(\n",
       "      (upconv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv): ConvBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (dec3): DecoderBlock(\n",
       "      (upconv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv): ConvBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (dec4): DecoderBlock(\n",
       "      (upconv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv): ConvBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (output): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "autoencoder.load_state_dict(torch.load('auto_encoder_wts/pretrained_wt_unet_model_epoch_114.pth'))\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3114068/3812214386.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_img_tensor = torch.tensor(test_img).float().unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test on a few images\n",
    "num = np.random.randint(0, len(img_array))\n",
    "test_img = img_array[num]\n",
    "test_img_tensor = torch.tensor(test_img).float().unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    pred = autoencoder(test_img_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If 'pred' is on GPU, move it to CPU\n",
    "pred_cpu = pred.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(pred_cpu)\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and reconstructed images\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_img.transpose(1, 2, 0)*255)\n",
    "plt.title('Original')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(pred.cpu().numpy()[0].transpose(1, 2, 0))\n",
    "plt.title('Reconstructed')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
